\section{问题介绍} % 张金龙

恶意评论分类是自然语言处理问题中经典的篇章级文本分类问题，给定一段文本，该任务旨在判断这段文本是否具有恶意倾向，包括人身攻击、人身侮辱等等。使用机器自动化地分类恶意评论有助于网站管理员维护健康的社区环境。

本问题提供了包含语句文本、标签以及恶意程度评分的训练集，以及只包含语句文本的测试集，训练集包含1048575个样本，测试集包含27074个样本。语句文本由一句话、多句话或多段构成，且普遍具有口语化、非正规表达的特点，如``???...You trying to hard...\#MAGA''等。标签中包含了43种特征，以及通过投票得出的对应特征值，如``rating''：approved，``funny''：1等，其中部分标签内容指出了语句文本中是否包含了可能引起攻击性的词汇，如``black''，``bisexual''等，以及具有明显倾向性的评价指标，如``severe\_toxicity''，``insult''等，但部分特征的残缺值较多。样本语句的恶意程度由[0,1]区间的评分值给出，分值越大，表明其恶意程度越强。因此，有效提取语句文本自身的信息以及标签信息，并在模型中结合使用，将有助于对语句文本的恶意程度进行准确评分。

在本文中，我们采用预训练BERT模型\cite{devlin2019bert}作为分类器的主体架构，以社区言论文本作为输入，使用给定的标注数据训练分类模型。我们将其看作文本二分类问题，首先使用预训练分词器对文本进行分词，然后输入到BERT模型提取文本特征，最后使用Softmax层计算该言论为恶意言论的预测概率。

由于恶意言论中往往会包含一些敏感身份词，例如``黑人''、``同性恋''或``犹太人''等词语。因此模型在训练时可能会学习到偏见，错误地将身份词与恶意言论关联起来，导致模型的预测不可靠。以往研究提出了多种多样的方法以减少模型在学习中产生的偏见，例如域对抗（DAT）\cite{ganin2016domain}、不变解释（InvRat）\cite{chang2020invariant}等等。

本文采用了不变解释方法\cite{chang2020invariant}中的思想，使用基于信息论方法的去偏算法减少模型的偏见。该方法首先对输入文本计算遮罩，以寻找能够合理解释模型推理的片段。然后使用两个不同的分类器来消除非合理解释对模型预测的影响，从而减少模型偏见。

在实验中，我们训练了BERT模型和使用了不变解释去偏算法的模型。其中微调后的预训练BERT模型在测试集上取得了93.41\%的准确率。然而使用不变解释去偏算法后的BERT模型的效果尚不理想，我们在实验部分对该现象进行了初步的分析和解释。

该工作的主要贡献总结如下：
\begin{itemize}
    \item 我们分析了恶意评论分类数据集，将其建模为文本二分类问题。
    \item 我们阐述并实现了基于信息论方法的去偏算法以消除模型在学习中产生的偏见。
    \item 经过微调的预训练BERT模型在测试集上取得了93.41\%的准确率。
\end{itemize}
